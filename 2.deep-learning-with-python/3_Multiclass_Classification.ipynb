{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.Multiclass-Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrAx9sNclawCPM5yZovTSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tecnocrata/artificial-intelligence-course/blob/master/2.deep-learning-with-python/3_Multiclass_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7NlBj_4ZNE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From chapter 3 and 3.5\n",
        "# Classifying newswires into 46 mutually exclusive topics\n",
        "# Importing Reuters dataset\n",
        "from keras.datasets import reuters\n",
        "# Words are ranked by how often they occur (in the training set) and only the num_words most frequent words are kept.\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data( num_words=10000)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJyfc-LCdtIy",
        "colab_type": "text"
      },
      "source": [
        "# Analyzing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7SuysBbdisd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8a26967c-babc-4060-a96f-14da51dcc779"
      },
      "source": [
        "# We have 8,982 training examples and 2,246 test examples\n",
        "print (len(train_data))\n",
        "print (len(test_data))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8982\n",
            "2246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Cf3dygeFHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6530ac7f-e487-4a99-83c9-b7aa912ec03e"
      },
      "source": [
        "# As with the IMDB reviews, each example is a list of integers (word indices):\n",
        "print(train_data[0])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX8JW544ebKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ad87985c-13cc-4315-c446-dc891485a523"
      },
      "source": [
        "# Again as with IMDB the previous array encodes the news\n",
        "def decode_review(review):\n",
        "  word_index = reuters.get_word_index()\n",
        "  reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "  # Decodes the review. Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”\n",
        "  decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in review])\n",
        "  return decoded_review\n",
        "decode_review (train_data[0])"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMWJNNehfdOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81b9b4ac-f84e-413e-bb1f-d280ff8b3b45"
      },
      "source": [
        "# The labels for the previous news is\n",
        "print (train_labels[0])\n",
        "# Como dije anteriormente hay 46 posibles labels/clases o salidas por cada noticia\n",
        "# En este caso se esta asignando el valor 3\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4TcJ4AgcVZ",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the data\n",
        "### One-hot encoding (train & test) inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dERaV3ZEf3Dp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "18ff63d5-c11b-40d4-c436-d0ee30802f45"
      },
      "source": [
        "import numpy as np\n",
        "#def vectorize_sequences(sequences, dimension=10000): \n",
        "def onehot_encode(sequences, dimension=10000): \n",
        "  # Creamos una matriz de ceros de cantidad de elementos x 10000\n",
        "  results = np.zeros((len(sequences), dimension)) \n",
        "  for i, sequence in enumerate(sequences):\n",
        "    # ponemos 1 solo en la posicion de ese numero\n",
        "    results[i, sequence] = 1\n",
        "  return results\n",
        "\n",
        "# Prepare data, encoding into 0s and 1s vector\n",
        "x_train = onehot_encode (train_data)\n",
        "x_test = onehot_encode(test_data)\n",
        "\n",
        "print (train_data[0])\n",
        "# Obteniendo elementos unicos\n",
        "print (set(train_data[0]))\n",
        "# Cuantos elementos unicos tengo\n",
        "print ('Unique elements ...', len(set(train_data[0])))\n",
        "print (x_train[0])\n",
        "unique, counts = np.unique(x_train[0], return_counts=True)\n",
        "print ('x train encoded contains 10000 input values')\n",
        "print (len(x_train[0]))\n",
        "# Observese que hay 57 1s (unos) dentro de un array de 10 mil. \n",
        "# Esto quiere decir que los anteriores 57 Unique elements, estan encodeados como 1s en ese array\n",
        "print ('there are only 57 ones')\n",
        "print (dict(zip(unique, counts)))\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
            "{1, 2, 258, 4, 5, 6, 7, 8, 9, 10, 11, 134, 132, 270, 15, 16, 272, 17, 19, 12, 22, 3095, 151, 25, 26, 154, 155, 29, 30, 32, 39, 43, 44, 48, 49, 52, 186, 447, 67, 197, 207, 209, 83, 84, 89, 90, 1245, 864, 482, 4579, 102, 1005, 109, 111, 369, 504, 124}\n",
            "Unique elements ... 57\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "x train encoded contains 10000 input values\n",
            "10000\n",
            "there are only 57 ones\n",
            "{0.0: 9943, 1.0: 57}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9TYLIiQiB-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note that there is a built-in way to do this in Keras, using to_categorical function\n",
        "from keras.utils.np_utils import to_categorical\n",
        "one_hot_train_labels = to_categorical(train_labels) \n",
        "one_hot_test_labels = to_categorical(test_labels)\n"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eLG4q18gUlk",
        "colab_type": "text"
      },
      "source": [
        "# Building the model architecture\n",
        "In the previous example, you used 16-dimensional intermediate layers, but a 16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as infor- mation bottlenecks, permanently dropping relevant information.\n",
        "For this reason you’ll use larger layers. Let’s go with 64 units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZB_hAy9h9K5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \n",
        "model.add(layers.Dense(64, activation='relu')) \n",
        "# we end the network with a Dense layer of size 46, because there are 46 possible classes\n",
        "# The last layer uses a softmax activation. It means the network will output a probability distribution over the 46 different output classes\n",
        "# In other words: The 46 scores will sum to 1.\n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra-WkFNGg7HZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The best loss function to use in this case is categorical_crossentropy. \n",
        "# It measures the distance between two probability distributions: here, \n",
        "# between the probability dis- tribution output by the network and the true distribution of the labels. \n",
        "# By minimizing the distance between these two distributions, \n",
        "# you train the network to output some- thing as close as possible to the true labels.\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5y75FkFh2Gj",
        "colab_type": "text"
      },
      "source": [
        "# Validating the approach\n",
        "In order to monitor during training the accuracy of the model on data it has never seen before, you’ll create a validation set by setting apart 10,000 samples from the original training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62fi378-h1Yi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "7fbf7cb6-de57-431e-fa02-ecbd9755a66c"
      },
      "source": [
        "x_val = x_train[:1000] \n",
        "partial_x_train = x_train[1000:]\n",
        "y_val = one_hot_train_labels[:1000] \n",
        "partial_y_train = one_hot_train_labels[1000:]\n",
        "\n",
        "# Now, let’s train the network for 20 epochs.\n",
        "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 1s 154us/step - loss: 2.6326 - accuracy: 0.4737 - val_loss: 1.7996 - val_accuracy: 0.6140\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 1s 163us/step - loss: 1.4497 - accuracy: 0.6971 - val_loss: 1.3207 - val_accuracy: 0.7060\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 1s 162us/step - loss: 1.0475 - accuracy: 0.7724 - val_loss: 1.1254 - val_accuracy: 0.7570\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 1s 159us/step - loss: 0.8206 - accuracy: 0.8222 - val_loss: 1.0186 - val_accuracy: 0.7870\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 1s 134us/step - loss: 0.6518 - accuracy: 0.8588 - val_loss: 0.9652 - val_accuracy: 0.7900\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 1s 145us/step - loss: 0.5224 - accuracy: 0.8908 - val_loss: 0.9159 - val_accuracy: 0.8050\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 1s 158us/step - loss: 0.4227 - accuracy: 0.9116 - val_loss: 0.8756 - val_accuracy: 0.8140\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 1s 139us/step - loss: 0.3449 - accuracy: 0.9261 - val_loss: 0.8717 - val_accuracy: 0.8100\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 0.2809 - accuracy: 0.9387 - val_loss: 0.8892 - val_accuracy: 0.8120\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 0.2394 - accuracy: 0.9445 - val_loss: 0.8998 - val_accuracy: 0.8080\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 0.2051 - accuracy: 0.9490 - val_loss: 0.9071 - val_accuracy: 0.8270\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 0.1834 - accuracy: 0.9531 - val_loss: 0.9071 - val_accuracy: 0.8140\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 1s 152us/step - loss: 0.1615 - accuracy: 0.9529 - val_loss: 0.9422 - val_accuracy: 0.8100\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 1s 137us/step - loss: 0.1465 - accuracy: 0.9578 - val_loss: 0.9847 - val_accuracy: 0.8070\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 0.1336 - accuracy: 0.9578 - val_loss: 0.9524 - val_accuracy: 0.8170\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 1s 136us/step - loss: 0.1338 - accuracy: 0.9546 - val_loss: 0.9974 - val_accuracy: 0.8140\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 1s 132us/step - loss: 0.1268 - accuracy: 0.9543 - val_loss: 1.0034 - val_accuracy: 0.8110\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 1s 153us/step - loss: 0.1177 - accuracy: 0.9574 - val_loss: 1.0417 - val_accuracy: 0.8110\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 1s 162us/step - loss: 0.1118 - accuracy: 0.9583 - val_loss: 1.0995 - val_accuracy: 0.7980\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 1s 162us/step - loss: 0.1121 - accuracy: 0.9588 - val_loss: 1.0631 - val_accuracy: 0.8060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6r975d2m03y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6a232271-1003-49b2-a008-52374a61b5bc"
      },
      "source": [
        "# Analyzin history.history\n",
        "history_dict = history.history\n",
        "print (history_dict.keys())\n",
        "print (len(history_dict['val_loss']))\n",
        "print (len(history_dict['val_accuracy']))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n",
            "20\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EklxgvOeladE",
        "colab_type": "text"
      },
      "source": [
        "### Plotting the training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZkwmrNTlC12",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ee039a0f-8901-4a59-843a-93822e6b49f7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss = history.history['loss'] \n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss') \n",
        "plt.title('Training and validation loss') \n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss') \n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# La curva de puntos es la perdida en el entrenamiento, a menor perdida mejor!\n",
        "# La curva solida es la perdida con los datos de validacion\n",
        "# Si observamos en el entreamiento la perdida se acerca a cero, lo q es bueno!\n",
        "# PERO cuando usamos el modelo con los datos de validacion, la perdida se dispara a partir de la 8-9 epoch"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtWl5eVYlpmT",
        "colab_type": "text"
      },
      "source": [
        "### Plotting the training and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWs976Xplrwe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "370954a1-9ce4-4e1c-8852-0e262be39c70"
      },
      "source": [
        "# Graficamos similarmente para los datos de precision (accurracy)\n",
        "plt.clf() #Clears the figure\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc') \n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc') \n",
        "plt.title('Training and validation accuracy') \n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss') \n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Esto es otra forma de verlo\n",
        "# Con los datos de entreamiento (puntos) cada vez hay mayor presicion (cerca a 1)\n",
        "# Con los datos de validacion (solida) a partir de la 6ta-9na epoch, no se mejora la precision\n",
        "# Esto es el overfiting"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7fYSxiQMrVO",
        "colab_type": "text"
      },
      "source": [
        "# Retraining a model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58znk7FXMq68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "48525020-fdc6-452f-c4e3-68a359109be7"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \n",
        "model.add(layers.Dense(64, activation='relu')) \n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy']) \n",
        "# Esta vez solamente 9 epochs\n",
        "model.fit(partial_x_train, partial_y_train, epochs=9, batch_size=512, validation_data=(x_val, y_val))\n",
        "# Probamos el modelo entrenado con los datos de prueba\n",
        "results = model.evaluate(x_test, one_hot_test_labels)\n",
        "results\n",
        "\n",
        "# La precision esta cercana al 80%"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/9\n",
            "7982/7982 [==============================] - 1s 162us/step - loss: 2.5588 - accuracy: 0.5291 - val_loss: 1.6989 - val_accuracy: 0.6500\n",
            "Epoch 2/9\n",
            "7982/7982 [==============================] - 1s 163us/step - loss: 1.3917 - accuracy: 0.7001 - val_loss: 1.3069 - val_accuracy: 0.7060\n",
            "Epoch 3/9\n",
            "7982/7982 [==============================] - 1s 166us/step - loss: 1.0504 - accuracy: 0.7749 - val_loss: 1.1600 - val_accuracy: 0.7440\n",
            "Epoch 4/9\n",
            "7982/7982 [==============================] - 1s 136us/step - loss: 0.8410 - accuracy: 0.8206 - val_loss: 1.0503 - val_accuracy: 0.7690\n",
            "Epoch 5/9\n",
            "7982/7982 [==============================] - 1s 145us/step - loss: 0.6770 - accuracy: 0.8537 - val_loss: 1.0013 - val_accuracy: 0.7840\n",
            "Epoch 6/9\n",
            "7982/7982 [==============================] - 1s 147us/step - loss: 0.5498 - accuracy: 0.8844 - val_loss: 0.9406 - val_accuracy: 0.8090\n",
            "Epoch 7/9\n",
            "7982/7982 [==============================] - 1s 156us/step - loss: 0.4442 - accuracy: 0.9055 - val_loss: 0.9508 - val_accuracy: 0.8020\n",
            "Epoch 8/9\n",
            "7982/7982 [==============================] - 1s 161us/step - loss: 0.3606 - accuracy: 0.9230 - val_loss: 0.8961 - val_accuracy: 0.8150\n",
            "Epoch 9/9\n",
            "7982/7982 [==============================] - 1s 152us/step - loss: 0.3036 - accuracy: 0.9308 - val_loss: 0.9346 - val_accuracy: 0.8050\n",
            "2246/2246 [==============================] - 0s 99us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0260190360798755, 0.7791629433631897]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1sfS8JfN3os",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5830bb3-a94c-480d-d199-d893688d20e3"
      },
      "source": [
        "import copy\n",
        "test_labels_copy = copy.copy(test_labels)\n",
        "np.random.shuffle(test_labels_copy)\n",
        "hits_array = np.array(test_labels) == np.array(test_labels_copy) \n",
        "float(np.sum(hits_array)) / len(test_labels)\n",
        "# Usando un clasificador randomico la precision es cercana al 18% - 19%\n",
        "# Esto quiere decir que el modelo anterior y su precision cercana al 80% es mucho mejor"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18210151380231523"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSz3nlLQPIWn",
        "colab_type": "text"
      },
      "source": [
        "# Generating predictions on new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1TAjVLwPMCe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "0aa039ba-d095-4bc2-fda5-b15544573478"
      },
      "source": [
        "# Vamos a usar el modelo en datos nuevos\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# Tenemos 2246 datos de prueba\n",
        "print (x_test.shape)\n",
        "# Como resultado de la prediccion tenemos 2246 predicciones, cada una con 46 posibles clases\n",
        "print (predictions.shape)\n",
        "\n",
        "print (predictions[:2])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2246, 10000)\n",
            "(2246, 46)\n",
            "[[1.57957795e-04 7.85357915e-05 5.65465234e-06 9.45796072e-01\n",
            "  3.90761904e-02 3.16084916e-05 1.81761308e-04 7.09448796e-05\n",
            "  1.72466121e-03 5.24415955e-05 2.24832274e-05 7.03380303e-03\n",
            "  7.54322537e-05 1.84312958e-04 2.93324265e-04 4.08330270e-05\n",
            "  3.31815245e-04 2.00982104e-05 7.88871694e-05 3.55502816e-05\n",
            "  1.74652936e-03 1.13044574e-03 3.25170113e-05 2.95032914e-05\n",
            "  9.11855568e-06 7.20847820e-05 5.24715324e-06 2.05301098e-04\n",
            "  2.17677716e-05 1.83567608e-04 2.52998580e-04 3.19192157e-04\n",
            "  7.39200595e-06 3.17830272e-05 9.32413241e-05 3.32658201e-05\n",
            "  1.38445088e-04 2.05781635e-05 2.08754482e-05 1.38004412e-04\n",
            "  1.89076945e-05 3.08933086e-05 1.00147745e-05 1.44320395e-04\n",
            "  9.44279600e-06 2.18853666e-06]\n",
            " [1.87831896e-03 1.01682730e-02 6.63932636e-02 2.29556426e-05\n",
            "  2.35371990e-04 1.04038296e-02 3.69010319e-04 2.08308775e-04\n",
            "  4.99708462e-04 4.97972779e-03 7.66133070e-01 4.27722419e-03\n",
            "  4.51810332e-03 4.58003866e-04 3.07362876e-04 1.27286278e-03\n",
            "  4.70571127e-03 1.42245863e-05 7.81325216e-04 8.93763732e-04\n",
            "  1.61871202e-02 1.91798928e-04 4.84188320e-04 1.70504602e-04\n",
            "  2.39362810e-02 3.03138536e-03 2.02431064e-03 2.75670202e-04\n",
            "  6.52030557e-02 2.75210368e-05 2.02630268e-04 6.86956337e-03\n",
            "  2.03995878e-04 1.16613564e-05 5.65261907e-05 1.01064383e-04\n",
            "  1.43067664e-04 3.49967537e-04 5.50135563e-04 2.81783170e-04\n",
            "  3.25565234e-05 3.32512252e-04 3.65892047e-05 1.91567859e-04\n",
            "  1.18547097e-04 4.65444900e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2LQOf1-QOrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64dc31e4-40f2-4f77-ca80-ce80f1f0a860"
      },
      "source": [
        "# Por ejemplo la prediccion para la primera entrada, tiene esas 46 salidas\n",
        "predictions[0].shape"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-kbfcmkQ0C3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c21ee24c-9b13-4802-bcaa-c9c652f9d8f2"
      },
      "source": [
        "# Como dijimos anteriormente, la suma de todas las 46 probabilidades, debe ser 1\n",
        "np.sum(predictions[0])"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpSm4y94RA41",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f805efe-2bdc-4d79-f8bf-e9d9e7037d5a"
      },
      "source": [
        "# Obtenemos cual es la clase mas probable, es decir con la mayor probabilidad entre todas\n",
        "np.argmax(predictions[0])"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaBhmIFjVq1C",
        "colab_type": "text"
      },
      "source": [
        "# Additional tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0O-9DwRP_kP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "fe6cf885-aff7-4b22-cd6a-deaa3fa9eed1"
      },
      "source": [
        "# Si queremos usar los inputs y outputs como numeros enteros, SIN usar one-hot encoding\n",
        "# Podemos usar esta funcion de perdida: sparse_categorical_crossentropy\n",
        "y_train = np.array(train_labels) \n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "x_val = x_train[:1000] \n",
        "partial_x_train = x_train[1000:]\n",
        "y_val = y_train[:1000] #one_hot_train_labels[:1000] \n",
        "partial_y_train = y_train[1000:] #one_hot_train_labels[1000:]\n",
        "\n",
        "# Esta es la arquitectura inicial!!!\n",
        "# model = models.Sequential()\n",
        "# model.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \n",
        "# model.add(layers.Dense(64, activation='relu')) \n",
        "# model.add(layers.Dense(46, activation='softmax'))\n",
        "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy']) \n",
        "# # Esta vez solamente 9 epochs\n",
        "# model.fit(partial_x_train, partial_y_train, epochs=9, batch_size=512, validation_data=(x_val, y_val))\n",
        "# # Probamos el modelo entrenado con los datos de prueba\n",
        "# results = model.evaluate(x_test, one_hot_test_labels)\n",
        "# results\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \n",
        "model.add(layers.Dense(64, activation='relu')) \n",
        "# # Note this won't affect the model output shape, it still outputs ten probability scores for each input sample.!!!!!!!!!!!\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.fit(partial_x_train, partial_y_train, epochs=9, batch_size=512, validation_data=(x_val, y_val))\n",
        "# Probamos el modelo entrenado con los datos de prueba\n",
        "results = model.evaluate(x_test, y_test) #one_hot_test_labels\n",
        "results\n",
        "\n",
        "# Obtenemos similares resultados cuando usamos one-hot encoding"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/9\n",
            "7982/7982 [==============================] - 1s 157us/step - loss: 2.6070 - acc: 0.5053 - val_loss: 1.7291 - val_acc: 0.6370\n",
            "Epoch 2/9\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 1.4152 - acc: 0.7058 - val_loss: 1.2990 - val_acc: 0.7140\n",
            "Epoch 3/9\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 1.0396 - acc: 0.7762 - val_loss: 1.1328 - val_acc: 0.7620\n",
            "Epoch 4/9\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 0.8204 - acc: 0.8232 - val_loss: 1.0428 - val_acc: 0.7830\n",
            "Epoch 5/9\n",
            "7982/7982 [==============================] - 1s 152us/step - loss: 0.6589 - acc: 0.8604 - val_loss: 0.9884 - val_acc: 0.7970\n",
            "Epoch 6/9\n",
            "7982/7982 [==============================] - 1s 140us/step - loss: 0.5282 - acc: 0.8899 - val_loss: 0.9443 - val_acc: 0.8070\n",
            "Epoch 7/9\n",
            "7982/7982 [==============================] - 1s 166us/step - loss: 0.4265 - acc: 0.9110 - val_loss: 0.9286 - val_acc: 0.8120\n",
            "Epoch 8/9\n",
            "7982/7982 [==============================] - 1s 163us/step - loss: 0.3468 - acc: 0.9258 - val_loss: 0.9079 - val_acc: 0.8150\n",
            "Epoch 9/9\n",
            "7982/7982 [==============================] - 1s 161us/step - loss: 0.2875 - acc: 0.9379 - val_loss: 0.9304 - val_acc: 0.8150\n",
            "2246/2246 [==============================] - 0s 125us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9924344706514109, 0.780053436756134]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TCxNqDMgmog",
        "colab_type": "text"
      },
      "source": [
        "# The importance of having sufficiently large intermediate layers\n",
        "You should avoid intermediate layers with many fewer than 46 hidden units. Now let’s see what happens when you introduce an information bottleneck by having intermediate layers that are significantly less than 46-dimensional: for example, 4-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHYUxT2Tgkjo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "c61b8f9c-c2e7-4233-cf5c-7fdd151fa131"
      },
      "source": [
        "# Volvemos a preparar los datos de entrenamiento por que se cambiaron en la anterior prueba\n",
        "x_val = x_train[:1000] \n",
        "partial_x_train = x_train[1000:]\n",
        "y_val = one_hot_train_labels[:1000] \n",
        "partial_y_train = one_hot_train_labels[1000:]\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \n",
        "model.add(layers.Dense(4, activation='relu')) \n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=128, validation_data=(x_val, y_val))\n",
        "\n",
        "# Probamos el modelo entrenado con los datos de prueba\n",
        "results = model.evaluate(x_test, one_hot_test_labels)\n",
        "results\n",
        "# Se observa que la precision solo alcanza el 70% - 71% esto es debido a que usamos solo neuronas en una capa intermedia"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 2s 210us/step - loss: 3.2181 - accuracy: 0.1654 - val_loss: 2.4674 - val_accuracy: 0.2420\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 2s 210us/step - loss: 1.8814 - accuracy: 0.5227 - val_loss: 1.6120 - val_accuracy: 0.5810\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 2s 214us/step - loss: 1.4007 - accuracy: 0.6059 - val_loss: 1.4510 - val_accuracy: 0.6100\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 2s 205us/step - loss: 1.1831 - accuracy: 0.6783 - val_loss: 1.3468 - val_accuracy: 0.6590\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 2s 220us/step - loss: 1.0277 - accuracy: 0.7403 - val_loss: 1.3106 - val_accuracy: 0.6880\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 2s 192us/step - loss: 0.9130 - accuracy: 0.7720 - val_loss: 1.3028 - val_accuracy: 0.7010\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 1s 169us/step - loss: 0.8251 - accuracy: 0.7889 - val_loss: 1.2826 - val_accuracy: 0.7060\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 2s 195us/step - loss: 0.7514 - accuracy: 0.7982 - val_loss: 1.2992 - val_accuracy: 0.7050\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 1s 186us/step - loss: 0.6880 - accuracy: 0.8128 - val_loss: 1.3309 - val_accuracy: 0.7080\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 1s 185us/step - loss: 0.6307 - accuracy: 0.8271 - val_loss: 1.3347 - val_accuracy: 0.7140\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 1s 178us/step - loss: 0.5805 - accuracy: 0.8441 - val_loss: 1.3869 - val_accuracy: 0.7100\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 2s 221us/step - loss: 0.5380 - accuracy: 0.8578 - val_loss: 1.4262 - val_accuracy: 0.7210\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 2s 193us/step - loss: 0.5033 - accuracy: 0.8670 - val_loss: 1.4263 - val_accuracy: 0.7200\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 2s 193us/step - loss: 0.4695 - accuracy: 0.8771 - val_loss: 1.4889 - val_accuracy: 0.7270\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 2s 223us/step - loss: 0.4399 - accuracy: 0.8844 - val_loss: 1.5162 - val_accuracy: 0.7220\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 1s 182us/step - loss: 0.4190 - accuracy: 0.8882 - val_loss: 1.5969 - val_accuracy: 0.7240\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 2s 191us/step - loss: 0.3958 - accuracy: 0.8949 - val_loss: 1.6581 - val_accuracy: 0.7220\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 1s 171us/step - loss: 0.3784 - accuracy: 0.8976 - val_loss: 1.6895 - val_accuracy: 0.7200\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 1s 171us/step - loss: 0.3629 - accuracy: 0.9013 - val_loss: 1.7762 - val_accuracy: 0.7140\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 1s 169us/step - loss: 0.3486 - accuracy: 0.9022 - val_loss: 1.8064 - val_accuracy: 0.7100\n",
            "2246/2246 [==============================] - 0s 221us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.1288946150459567, 0.7079252004623413]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLj3bdApiJ8F",
        "colab_type": "text"
      },
      "source": [
        "# Conclusions\n",
        "- If you’re trying to classify data points among N classes, your network should end with a Dense layer of size N.\n",
        "- In a single-label, multiclass classification problem, your network should end with a softmax activation so that it will output a probability distribution over the N output classes.\n",
        "- Categorical crossentropy is almost always the loss function you should use for such problems. It minimizes the distance between the probability distributions output by the network and the true distribution of the targets.\n",
        "- There are two ways to handle labels in multiclass classification:\n",
        "  * Encoding the labels via categorical encoding (also known as one-hot encoding) and using categorical_crossentropy as a loss function\n",
        "  * Encoding the labels as integers and using the sparse_categorical_crossentropy\n",
        "loss function\n",
        "- If you need to classify data into a large number of categories, you should avoid\n",
        "creating information bottlenecks in your network due to intermediate layers that are too small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh-pUURsh7AG",
        "colab_type": "text"
      },
      "source": [
        "# Homework\n",
        "- Try using larger or smaller layers: 32 units, 128 units, and so on.\n",
        "- You used two hidden layers. Now try using a single hidden layer, or three hid-\n",
        "den layers."
      ]
    }
  ]
}