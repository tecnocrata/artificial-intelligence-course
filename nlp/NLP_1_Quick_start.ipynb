{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-1.Quick start.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMb/GYbOmnz55jrsORVD+s2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tecnocrata/artificial-intelligence-course/blob/master/nlp/NLP_1_Quick_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vsW1RsnHwju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEXNLj1xH7ry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f793b877-b559-4ceb-8257-49f3c3fd485c"
      },
      "source": [
        "# Descargar corpus & corpora\n",
        "# Corpus en ES\n",
        "nltk.download('cess_esp')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq37Y6dZISyL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a74153fc-d3fd-406b-caf7-903b448684bd"
      },
      "source": [
        "# La libreria de python es 're'\n",
        "import re\n",
        "# Es texto tokenizado, titulares de noticias en espanol\n",
        "corpus = nltk.corpus.cess_esp.sents()\n",
        "print (corpus)\n",
        "#tiene esta cantidad de titulares\n",
        "print (len(corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]\n",
            "6030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CoelmaWJL2u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aa987fa6-de53-4501-fc77-07ec222fec35"
      },
      "source": [
        "# Aplanamos la lista\n",
        "# estamos sacando las palabras (w) y las estamos poniendo en una unica lista\n",
        "flatten = [w for l in corpus for w in l]\n",
        "# son 192685 tokens\n",
        "print (len(flatten))\n",
        "# solo quiero los primeros 20 elementos\n",
        "print (flatten[:20])\n",
        "# esto-> w for l in corpus for w in l .......es lo mismo que\n",
        "# flatten = []\n",
        "# for l in corpus:\n",
        "#   for w in l:\n",
        "#     flatten.append(w)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "192685\n",
            "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHB0B4_SLCYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "84dc6add-26dc-41ce-9680-66f2d6b933b0"
      },
      "source": [
        "# re.search(p, s)\n",
        "# p es el patron de busqueda\n",
        "# s es la cadena de texto\n",
        "# que contenga es\n",
        "arr = [w for w in flatten if re.search ('es', w)]\n",
        "print (arr[:5])\n",
        "\n",
        "# Que termine en es\n",
        "arr = [w for w in flatten if re.search ('es$', w)]\n",
        "print (arr[:5])\n",
        "\n",
        "# Que empiece en es\n",
        "arr = [w for w in flatten if re.search ('^es', w)]\n",
        "print (arr[:5])\n",
        "\n",
        "# Usando el rango\n",
        "# [a-z] que el caracter este entre a y z\n",
        "# Al princiopio de la cadena puede existir cualquiera dentro de g,h e i\n",
        "arr = [w for w in flatten if re.search ('^[ghi]', w)]\n",
        "print (arr[:5])\n",
        "\n",
        "# Las clausuras\n",
        "# * = repetir 0 o mas veces\n",
        "# + = repetir una o mas veces\n",
        "# Que empiece con una secuencia no, cero o mas veces\n",
        "arr = [w for w in flatten if re.search ('^(no)*', w)]\n",
        "print (arr[:5])\n",
        "\n",
        "# Que empiece con una secuencia no, una o mas veces\n",
        "arr = [w for w in flatten if re.search ('^(no)+', w)]\n",
        "print (arr[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['estatal', 'jueves', 'empresa', 'centrales', 'francesa']\n",
            "['jueves', 'centrales', 'millones', 'millones', 'dólares']\n",
            "['estatal', 'es', 'esta', 'esta', 'eso']\n",
            "['grupo', 'hoy', 'gas', 'gas', 'intervendrá']\n",
            "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-']\n",
            "['norte', 'no', 'no', 'noche', 'no']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1TB7FYb5P3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "859d1414-0325-4855-9e2f-bbb6be18d7c0"
      },
      "source": [
        "# Interpreta \\n como un caracter de escape, generando una nueva linea\n",
        "print('esta es \\n una prueba')\n",
        "# Interpreta literalmente la cadena\n",
        "print(r'esta es \\n una prueba')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "esta es \n",
            " una prueba\n",
            "esta es \\n una prueba\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmy1g5Vi6a6A",
        "colab_type": "text"
      },
      "source": [
        "# Tokenizacion con Expresiones Regulares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQXbrJy85rdP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fd2e621c-610b-460b-f22d-4902259996ec"
      },
      "source": [
        "texto = \"\"\" Cuando sea el rey del mundo  (imaginaba él en su cabeza) no tendré que  preocuparme por estas bobadas. \n",
        "            Era solo un niño de 7 años, pero pensaba que podría ser cualquier cosa que su imaginación le permitiera visualizar en su cabeza ...\"\"\"\n",
        "print(texto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Cuando sea el rey del mundo  (imaginaba él en su cabeza) no tendré que  preocuparme por estas bobadas. \n",
            "            Era solo un niño de 7 años, pero pensaba que podría ser cualquier cosa que su imaginación le permitiera visualizar en su cabeza ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1MD5PQT5ZST",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "120e4948-00cb-434c-9a29-17ff66de0bc5"
      },
      "source": [
        "# Tokenizar la cadena por espacios vacios\n",
        "print (re.split (r' ', texto))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', '', '(imaginaba', 'él', 'en', 'su', 'cabeza)', 'no', 'tendré', 'que', '', 'preocuparme', 'por', 'estas', 'bobadas.', '\\n', '', '', '', '', '', '', '', '', '', '', '', 'Era', 'solo', 'un', 'niño', 'de', '7', 'años,', 'pero', 'pensaba', 'que', 'podría', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginación', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIydqNjC6-Sq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e04435d0-9b8c-4b06-9db3-0b1159a51a18"
      },
      "source": [
        "# Tokenizar la cadena por una expresion regular separando por espacios, tabuladores y saltos de linea\n",
        "print (re.split (r'[ \\t\\n]+', texto))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', '(imaginaba', 'él', 'en', 'su', 'cabeza)', 'no', 'tendré', 'que', 'preocuparme', 'por', 'estas', 'bobadas.', 'Era', 'solo', 'un', 'niño', 'de', '7', 'años,', 'pero', 'pensaba', 'que', 'podría', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginación', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhiPQY7C7mjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "56651eab-6b2f-448f-e8d6-c5ac8f6fedc2"
      },
      "source": [
        "# Tokenizar la cadena por una expresion regular separando por espacios, tabuladores, saltos de linea y palabras\n",
        "print (re.split (r'[ \\W\\t\\n]+', texto))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', 'imaginaba', 'él', 'en', 'su', 'cabeza', 'no', 'tendré', 'que', 'preocuparme', 'por', 'estas', 'bobadas', 'Era', 'solo', 'un', 'niño', 'de', '7', 'años', 'pero', 'pensaba', 'que', 'podría', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginación', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAe9LmkH8CZe",
        "colab_type": "text"
      },
      "source": [
        "# Tokenizacion usando MLTK\n",
        "La tokenizacion personalizada, no es suficiente por ello usamos NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXhR_sV27F7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dee23cde-fc7a-41d8-b801-ec5072d277a0"
      },
      "source": [
        "# nuestra antigua regex no funciona en este caso: \n",
        "texto = 'En los E.U. esa postal vale $15.50 ...'\n",
        "print(re.split(r'[ \\W\\t\\n]+', texto))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['En', 'los', 'E', 'U', 'esa', 'postal', 'vale', '15', '50', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA0ZMWyP8YwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a2f14c7-e323-4d39-8993-384b8063a8c0"
      },
      "source": [
        "# Es necesaria una expresion regular mas compleja y el uso de nltk\n",
        "# Esta expresion selecciona las palabras y es practicamente\n",
        "# lo opuesto a la forma de hacer la anterior tokenizacion\n",
        "# en esta buscamos las palabras, en la anterior solo separabamos \n",
        "# las palabras en funcion a la expresion regular\n",
        "pattern = r'''(?x)                 # set flag to allow verbose regexps\n",
        "              (?:[A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n",
        "              | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
        "              | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
        "              | \\.\\.\\.             # ellipsis\n",
        "              | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
        "'''\n",
        "nltk.regexp_tokenize(texto, pattern)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['En', 'los', 'E.U.', 'esa', 'postal', 'vale', '$15.50', '...']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m82SKTw_zH14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}